{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd8332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c391e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('data/raw/reviews.csv')\n",
    "print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b467ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (6327, 23)\n",
      "Shape after cleaning rows: (6232, 23)\n"
     ]
    }
   ],
   "source": [
    "# Initial Cleaning (Rows)\n",
    "# Drop missing text and duplicates as identified in EDA\n",
    "print(f\"Original shape: {data.shape}\")\n",
    "data.dropna(subset=['reviewText', 'rating'], inplace=True)\n",
    "data.drop_duplicates(subset=['reviewText'], inplace=True)\n",
    "print(f\"Shape after cleaning rows: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24be1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Engineering (The Target Variable)\n",
    "# Filter out 3-star reviews to remove ambiguity\n",
    "data = data[data['rating'] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d409a112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after cleaning: (5820, 24)\n",
      "Class Distribution:\n",
      " sentiment\n",
      "1    5607\n",
      "0     213\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sy/vptgh1ln55ggff5yxg_t7tmm0000gn/T/ipykernel_1974/2638079195.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sentiment'] = data['rating'].apply(lambda x: 1 if x > 3 else 0)\n"
     ]
    }
   ],
   "source": [
    "# Create Binary Sentiment: 1 = Positive (4-5 stars), 0 = Negative (1-2 stars)\n",
    "data['sentiment'] = data['rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "print(f\"Shape after cleaning: {data.shape}\")\n",
    "print(\"Class Distribution:\\n\", data['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157694e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of cleaning the text manually, we will wrap our cleaning logic into a Scikit-Learn Transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom Transformer to clean raw text.\n",
    "    Compatible with Scikit-Learn Pipelines.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # just return self\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X is a pandas Series or list of text\n",
    "        # We apply the cleaning function to every item\n",
    "        return [self._clean_text(text) for text in X]\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        # 1. Lowercase\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # 2. Remove HTML tags (e.g., <br /> common in Amazon data)\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "        # 3. Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # 4. Remove Punctuation\n",
    "        # This translation table replaces every punctuation mark with None\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # 5. Remove numbers (optional, usually good for sentiment)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5508eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline constructed successfully.\n"
     ]
    }
   ],
   "source": [
    "#Text Vectorization (From Words to Numbers)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the Preprocessing Pipeline\n",
    "# Steps:\n",
    "# 1. 'cleaner': Runs our custom TextCleaner\n",
    "# 2. 'tfidf': Converts text to numbers\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('cleaner', TextCleaner()),\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=5000,      # Only keep top 5000 words (reduces noise)\n",
    "        stop_words='english',   # Remove common words (the, a, an)\n",
    "        ngram_range=(1, 2)      # Capture \"not good\" as a phrase (Bigrams)\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Pipeline constructed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e20ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 4656 reviews\n",
      "Testing Data: 1164 reviews\n",
      "\n",
      "Dry run of preprocessing pipeline on sample data...\n",
      "Vector shape (Rows, Features): (100, 3891)\n"
     ]
    }
   ],
   "source": [
    "# Define Features (X) and Target (y)\n",
    "X = data['reviewText']\n",
    "y = data['sentiment']\n",
    "\n",
    "# 2. Stratified Split\n",
    "# 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # This ensures balanced classes in split\n",
    ")\n",
    "\n",
    "print(f\"Training Data: {len(X_train)} reviews\")\n",
    "print(f\"Testing Data: {len(X_test)} reviews\")\n",
    "\n",
    "# Verify transformation\n",
    "# Let's run the training data through the pipeline to check dimensions\n",
    "# (Note: In Part 4, we will actually 'fit' this. Here we just check.)\n",
    "print(\"\\nDry run of preprocessing pipeline on sample data...\")\n",
    "sample_vector = preprocessing_pipeline.fit_transform(X_train[:100])\n",
    "print(f\"Vector shape (Rows, Features): {sample_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf001b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/processed/preprocessed_train.csv, preprocessed_test.csv, preprocessed_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned & split datasets for modeling\n",
    "import os\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# Use the pipeline's TextCleaner to produce cleaned text columns\n",
    "cleaner = preprocessing_pipeline.named_steps['cleaner']\n",
    "\n",
    "# Clean train / test text (X_train and X_test are pandas Series)\n",
    "clean_train = cleaner.transform(X_train)\n",
    "clean_test = cleaner.transform(X_test)\n",
    "clean_full = cleaner.transform(X)  # optional: full dataset cleaned\n",
    "\n",
    "# Build DataFrames to save\n",
    "df_train = pd.DataFrame({\n",
    "    'reviewText': X_train.values,\n",
    "    'clean_reviewText': clean_train,\n",
    "    'sentiment': y_train.values\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'reviewText': X_test.values,\n",
    "    'clean_reviewText': clean_test,\n",
    "    'sentiment': y_test.values\n",
    "})\n",
    "\n",
    "df_full = pd.DataFrame({\n",
    "    'reviewText': X.values,\n",
    "    'clean_reviewText': clean_full,\n",
    "    'sentiment': y.values\n",
    "})\n",
    "\n",
    "# Write CSVs to data/processed\n",
    "df_train.to_csv('data/processed/preprocessed_train.csv', index=False)\n",
    "df_test.to_csv('data/processed/preprocessed_test.csv', index=False)\n",
    "df_full.to_csv('data/processed/preprocessed_full.csv', index=False)\n",
    "\n",
    "print(\"Saved: data/processed/preprocessed_train.csv, preprocessed_test.csv, preprocessed_full.csv\")\n",
    "# ...existing code..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
